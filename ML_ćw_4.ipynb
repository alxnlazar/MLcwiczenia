{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "a0179633",
      "metadata": {
        "id": "a0179633"
      },
      "source": [
        "**Zadanie1**: Jak minimaln i maksymaln warto mo偶e przyjmowa wsp贸czynnik Giniego? Uzasadnij. Co to wtedy oznacza?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9080d3b8",
      "metadata": {
        "id": "9080d3b8"
      },
      "source": [
        "Wsp贸czynnik Giniego mieci si w przedziale od 0 do 1. Warto 0 oznacza, 偶e grupa jest rozdzielona cakowicie, tj. mamy punkt czysty. Poprzez warto 1 rozumie si, 偶e elementy s bardzo zr贸偶nicowane i nale偶 do r贸偶nych hipotetycznych grup. W toerii d偶ymy do osignicia punkt贸w czystych, czyli zmniejszenia wsp贸czynnika Giniego."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c9c36c20",
      "metadata": {
        "id": "c9c36c20"
      },
      "source": [
        "**Zadanie2**: Uzasadnij warto entropii w wierzchoku (u samej g贸ry) poprzez bezporednie obliczenia."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b899871",
      "metadata": {
        "id": "6b899871",
        "outputId": "ffd6f777-1ae5-4886-de66-361dfd232a3d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1.5799176782552726"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import math\n",
        "r1= 34/105\n",
        "r2= 32/105\n",
        "r3= 39/105\n",
        "log2r1= math.log2(r1)\n",
        "log2r2= math.log2(r2)\n",
        "log2r3= math.log2(r3)\n",
        "entropia= (-r1*log2r1)+(-r2*log2r2)+(-r3*log2r3)\n",
        "entropia"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "25a9034b",
      "metadata": {
        "id": "25a9034b"
      },
      "source": [
        "**Zadanie3**: Uzasadnij warto indeksu Giniego w wierzchoku (u samej g贸ry) poprzez bezporednie obliczenia."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "73659bb2",
      "metadata": {
        "id": "73659bb2",
        "outputId": "f4c44fce-7412-46f0-d19c-1ad41d784a55"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.6643083900226757"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "gini= 1-r1**2-r2**2-r3**2\n",
        "gini"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b7ea2ce",
      "metadata": {
        "id": "2b7ea2ce"
      },
      "source": [
        "**Zadanie4**: Przeczytaj https://predictivesolutions.pl/jak-udoskonalic-algorytm-drzew-decyzyjnych. Sporzd藕 odpowiedni notatk."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "50eea307",
      "metadata": {
        "id": "50eea307"
      },
      "source": [
        "Notatka zaczona w pliku pdf :)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e6ef2cca",
      "metadata": {
        "id": "e6ef2cca"
      },
      "source": [
        "**Zadanie5**: Modele lasu drzew decyzyjnych czsto wykorzystuj metod modyfikowania danych treningowych - agregacj bootstrapow (bootstraping). Polega ona na wielokrotnym losowaniu ze zwracaniem. Je偶eli takie losowanie powt贸zymy    razy, otrzymamy    elementowy zbi贸r danych treningowych, w kt贸rym cz przypadk贸w bdzie si powtarza. Poka偶, 偶e dla du偶ych    pr贸ba bdzie zawieraa rednio  63%  przypadk贸w z orginalnego zbioru."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "746b270b",
      "metadata": {
        "id": "746b270b",
        "outputId": "bf9a756d-605f-4522-92ca-d93bd514c274"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.6347"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x= range(10000)\n",
        "import random\n",
        "y= random.choices(x, k=10000)\n",
        "len(set(y))/10000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e9ebb317",
      "metadata": {
        "id": "e9ebb317",
        "outputId": "d3dbcc2c-3858-482f-bb55-26b29e32bbcd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.6323045752290363"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "n=1000\n",
        "1-(1-1/n)**n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "46725ff6",
      "metadata": {
        "id": "46725ff6"
      },
      "source": [
        "Miaam odpowiedzie na pytanie dlaczego tak wychodzi z tego wzoru. Wic najpierw zajam si (1-1/n)**n. Jako, 偶e jest to liczba jest operacja na liczbach n-tych obliczyam granic z n d偶acego do nieskoczonoci. Wyszo mi 1/e. Pod e podo偶yam 2,72. Z kocowego obliczenia wychodzio 1,72/2,72=0.632 :)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ef63fccc",
      "metadata": {
        "id": "ef63fccc"
      },
      "source": [
        "Dodatkowo wa偶ne jest max_features, kt贸re ustala t losowo. Jak wysoka to oznacza, 偶e drzewa bd do siebie podobne i bd atwo dopasowywa si do danych. Niska warto z kolei oznacza, 偶e drzewa bd zupenie inne i 偶e ka偶de bdzie musiao by do gbokie, aby dobrze si dopasowa do danych."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "296b1ad9",
      "metadata": {
        "id": "296b1ad9"
      },
      "source": [
        "**Zadanie6**: Rozwa偶 dane zawierajce nag贸wki, przy czym s to fake newsy (1298) oraz prawdziwe tytuy (1968). Jeden nag贸wek to jeden rekord.\n",
        "Wyznacz czstoci wystpowania wszystkich s贸w ze wszystkich nag贸wk贸w.\n",
        "\n",
        "Jakie sowa (opr贸cz stopwords贸w) najczciej pojawiay si w realnych a jakie w faszywych nag贸wkach?\n",
        "\n",
        "Bdziemy tworzy klasyfiaktor dla tych danych w oparciu o regresj logistyczn oraz drzewa losowe/lasy losowe. Ka偶dy nag贸wek bdzie reprezentowany w postaci wektora zer i jedynek w zale偶noci od wystpowania danego sowa (dugo wektora = liczba wszystkich unikatowych s贸w, mo偶e warto jednak zawzi? albo potraktowa jako hiperparametr). Podziel dane na 3 grupy: 70% zbi贸r treningowy, 15% zbi贸r walidacyjny, 15% zbi贸r testowy. Przetestuj r贸偶ne zestawy hiperparametr贸w na zbiorze walidacyjnym. Skomentuj otrzymane wyniki.\n",
        "\n",
        "露"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "052bd1b7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 450
        },
        "id": "052bd1b7",
        "outputId": "e45ebd8c-11fb-448c-b446-779ca41b9769"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train:\n",
            "<1x2 sparse matrix of type '<class 'numpy.int64'>'\n",
            "\twith 2 stored elements in Compressed Sparse Row format>\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-5326382125ea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m#stop words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mENGLISH_STOP_WORDS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mvect2\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"english\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0mvect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1281\u001b[0m         \"\"\"\n\u001b[1;32m   1282\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_warn_for_unused_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1283\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1284\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1342\u001b[0m             )\n\u001b[1;32m   1343\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmax_doc_count\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mmin_doc_count\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1344\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"max_df corresponds to < documents than min_df\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1345\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmax_features\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1346\u001b[0m                 \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sort_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: max_df corresponds to < documents than min_df"
          ]
        }
      ],
      "source": [
        "#dziaam na pliku real\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "vect= CountVectorizer()\n",
        "real=['real.txt']\n",
        "vect.fit(real) #tokenizacja\n",
        "bag_of_words= vect.transform(real) #tworzenie worka s贸w\n",
        "\n",
        "def data_split_group(data): #dziel dane na trzy grupy\n",
        "  train = data[:round(0.7*len(data))]\n",
        "  test = data[round(0.7*len(data)):round(0.15*len(data))]\n",
        "  val = data[round(0.15*len(data)):]\n",
        "data_split_group(real)\n",
        "\n",
        "vect= CountVectorizer().fit(real) #przetwarzam dane\n",
        "X_train= vect.transform(real)\n",
        "print(\"X_train:\\n{}\".format(repr(X_train))) #hmmm co za maa ta macierz...\n",
        "\n",
        "#stop words\n",
        "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
        "vect2= CountVectorizer(min_df=0.5, stop_words=\"english\").fit(real)\n",
        "vect.transform(real) #tu nie mog nic sensownego uzyska, poniewa偶 musi by bd na etapie tokenizacji\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fDJeB4rbd8tB"
      },
      "id": "fDJeB4rbd8tB",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}