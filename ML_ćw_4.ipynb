{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "a0179633",
      "metadata": {
        "id": "a0179633"
      },
      "source": [
        "**Zadanie1**: Jaką minimalną i maksymalną wartość może przyjmować współczynnik Giniego? Uzasadnij. Co to wtedy oznacza?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9080d3b8",
      "metadata": {
        "id": "9080d3b8"
      },
      "source": [
        "Współczynnik Giniego mieści się w przedziale od 0 do 1. Wartość 0 oznacza, że grupa jest rozdzielona całkowicie, tj. mamy punkt czysty. Poprzez wartość 1 rozumie się, że elementy są bardzo zróżnicowane i należą do różnych hipotetycznych grup. W toerii dążymy do osiągnięcia punktów czystych, czyli zmniejszenia współczynnika Giniego."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c9c36c20",
      "metadata": {
        "id": "c9c36c20"
      },
      "source": [
        "**Zadanie2**: Uzasadnij wartość entropii w wierzchołku (u samej góry) poprzez bezpośrednie obliczenia."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b899871",
      "metadata": {
        "id": "6b899871",
        "outputId": "ffd6f777-1ae5-4886-de66-361dfd232a3d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1.5799176782552726"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import math\n",
        "r1= 34/105\n",
        "r2= 32/105\n",
        "r3= 39/105\n",
        "log2r1= math.log2(r1)\n",
        "log2r2= math.log2(r2)\n",
        "log2r3= math.log2(r3)\n",
        "entropia= (-r1*log2r1)+(-r2*log2r2)+(-r3*log2r3)\n",
        "entropia"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "25a9034b",
      "metadata": {
        "id": "25a9034b"
      },
      "source": [
        "**Zadanie3**: Uzasadnij wartość indeksu Giniego w wierzchołku (u samej góry) poprzez bezpośrednie obliczenia."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "73659bb2",
      "metadata": {
        "id": "73659bb2",
        "outputId": "f4c44fce-7412-46f0-d19c-1ad41d784a55"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.6643083900226757"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "gini= 1-r1**2-r2**2-r3**2\n",
        "gini"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b7ea2ce",
      "metadata": {
        "id": "2b7ea2ce"
      },
      "source": [
        "**Zadanie4**: Przeczytaj https://predictivesolutions.pl/jak-udoskonalic-algorytm-drzew-decyzyjnych. Sporządź odpowiednią notatkę."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "50eea307",
      "metadata": {
        "id": "50eea307"
      },
      "source": [
        "Notatka załączona w pliku pdf :)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e6ef2cca",
      "metadata": {
        "id": "e6ef2cca"
      },
      "source": [
        "**Zadanie5**: Modele lasu drzew decyzyjnych często wykorzystują metodę modyfikowania danych treningowych - agregację bootstrapową (bootstraping). Polega ona na wielokrotnym losowaniu ze zwracaniem. Jeżeli takie losowanie powtózymy  𝑛  razy, otrzymamy  𝑛  elementowy zbiór danych treningowych, w którym część przypadków będzie się powtarzać. Pokaż, że dla dużych  𝑛  próba będzie zawierała średnio  63%  przypadków z orginalnego zbioru."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "746b270b",
      "metadata": {
        "id": "746b270b",
        "outputId": "bf9a756d-605f-4522-92ca-d93bd514c274"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.6347"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x= range(10000)\n",
        "import random\n",
        "y= random.choices(x, k=10000)\n",
        "len(set(y))/10000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e9ebb317",
      "metadata": {
        "id": "e9ebb317",
        "outputId": "d3dbcc2c-3858-482f-bb55-26b29e32bbcd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.6323045752290363"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "n=1000\n",
        "1-(1-1/n)**n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "46725ff6",
      "metadata": {
        "id": "46725ff6"
      },
      "source": [
        "Miałam odpowiedzieć na pytanie dlaczego tak wychodzi z tego wzoru. Więc najpierw zajęłam się (1-1/n)**n. Jako, że jest to liczba jest operacja na liczbach n-tych obliczyłam granicę z n dążacego do nieskończoności. Wyszło mi 1/e. Pod e podłożyłam 2,72. Z końcowego obliczenia wychodziło 1,72/2,72=0.632 :)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ef63fccc",
      "metadata": {
        "id": "ef63fccc"
      },
      "source": [
        "Dodatkowo ważne jest max_features, które ustala tą losowość. Jak wysoka to oznacza, że drzewa będą do siebie podobne i będą łatwo dopasowywać się do danych. Niska wartość z kolei oznacza, że drzewa będą zupełnie inne i że każde będzie musiało być dość głębokie, aby dobrze się dopasować do danych."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "296b1ad9",
      "metadata": {
        "id": "296b1ad9"
      },
      "source": [
        "**Zadanie6**: Rozważ dane zawierające nagłówki, przy czym są to fake newsy (1298) oraz prawdziwe tytuły (1968). Jeden nagłówek to jeden rekord.\n",
        "Wyznacz częstości występowania wszystkich słów ze wszystkich nagłówków.\n",
        "\n",
        "Jakie słowa (oprócz stopwordsów) najczęściej pojawiały się w realnych a jakie w fałszywych nagłówkach?\n",
        "\n",
        "Będziemy tworzyć klasyfiaktor dla tych danych w oparciu o regresję logistyczną oraz drzewa losowe/lasy losowe. Każdy nagłówek będzie reprezentowany w postaci wektora zer i jedynek w zależności od występowania danego słowa (długość wektora = liczba wszystkich unikatowych słów, może warto jednak zawęzić? albo potraktować jako hiperparametr). Podziel dane na 3 grupy: 70% zbiór treningowy, 15% zbiór walidacyjny, 15% zbiór testowy. Przetestuj różne zestawy hiperparametrów na zbiorze walidacyjnym. Skomentuj otrzymane wyniki.\n",
        "\n",
        "¶"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "052bd1b7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 450
        },
        "id": "052bd1b7",
        "outputId": "e45ebd8c-11fb-448c-b446-779ca41b9769"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train:\n",
            "<1x2 sparse matrix of type '<class 'numpy.int64'>'\n",
            "\twith 2 stored elements in Compressed Sparse Row format>\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-5326382125ea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m#stop words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mENGLISH_STOP_WORDS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mvect2\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"english\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0mvect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1281\u001b[0m         \"\"\"\n\u001b[1;32m   1282\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_warn_for_unused_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1283\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1284\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1342\u001b[0m             )\n\u001b[1;32m   1343\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmax_doc_count\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mmin_doc_count\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1344\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"max_df corresponds to < documents than min_df\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1345\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmax_features\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1346\u001b[0m                 \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sort_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: max_df corresponds to < documents than min_df"
          ]
        }
      ],
      "source": [
        "#działam na pliku real\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "vect= CountVectorizer()\n",
        "real=['real.txt']\n",
        "vect.fit(real) #tokenizacja\n",
        "bag_of_words= vect.transform(real) #tworzenie worka słów\n",
        "\n",
        "def data_split_group(data): #dzielę dane na trzy grupy\n",
        "  train = data[:round(0.7*len(data))]\n",
        "  test = data[round(0.7*len(data)):round(0.15*len(data))]\n",
        "  val = data[round(0.15*len(data)):]\n",
        "data_split_group(real)\n",
        "\n",
        "vect= CountVectorizer().fit(real) #przetwarzam dane\n",
        "X_train= vect.transform(real)\n",
        "print(\"X_train:\\n{}\".format(repr(X_train))) #hmmm coś za mała ta macierz...\n",
        "\n",
        "#stop words\n",
        "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
        "vect2= CountVectorizer(min_df=0.5, stop_words=\"english\").fit(real)\n",
        "vect.transform(real) #tu nie mogę nic sensownego uzyskać, ponieważ musi być błąd na etapie tokenizacji\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fDJeB4rbd8tB"
      },
      "id": "fDJeB4rbd8tB",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}