{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Zadanie1: Zastanów się dlaczego funkcja kosztu MSE może nie być najlepszym pomysłem dla tego problemu?"
      ],
      "metadata": {
        "id": "1Hoo_VQjWHp3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "MSE jest stosowany w przypadku odwzorowań liniowych. Przy założeniu, że odwzorowujemy obraz 1:1 jego zastosowanie ma teoretycznie sens. Jednakże, może być obecnych wiele szumów, a przy nim mamy pojedyńczy wektor wyjściowy. To tak jakby przywidywać coś binanego. Dodatkowo kernele gaussowskie mają tendencję do znikającego gradientu.\n",
        "Lepsza zdaje się np. kernel inverse quadratic dla WAE-MMD, albo błąd rekonstrukcji."
      ],
      "metadata": {
        "id": "czsZbXdgWXSH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Zadanie2: Jaka jest różnica między funkcją ReLU i LeakyReLU. Jakie to ma konsekwencje?"
      ],
      "metadata": {
        "id": "FfZTuT2ldq0m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "LReLU jest przekształceniem ReLU, służącej do rozwiązań nieliniowych. Z tą różnicą, iż LReLU nie przekształca wszystkie wartości ujemne jako zero, tylko używa małego parametru nachylenia pozostawiając informacje o ich ujemności. ReLU może okazać się gorszy, gdy część neuronów stanie się nieaktywna przez to że zeruje ujemne wartości. Może też przyczynić się do braku zbieżności "
      ],
      "metadata": {
        "id": "sOQRlG9BdtnV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Zadanie3: Jak zachowuje się funkcja kosztu w kolejnych epokach? Z czego to może wynikać? Czy to ułatwia czy utrudnia dostosowyanie hiperparametrów?"
      ],
      "metadata": {
        "id": "4VMkd61liRtr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Raz jest większa dla generatora, raz dla dyskryminatora. Nie ułatwia to z całą pewnością dobór hiperparametrów. \n",
        "Sytuacja wynika z tego, że uczymy adwersarialnie, tak więc funkcja kosztu dopasuwuje się do każdego kroku."
      ],
      "metadata": {
        "id": "DmQxcVKzlqfc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Zadanie4: Generator może realizować swój cel ucząć się bardzo dobrze generować tylko pewien podzbiór danych (np jedną cyfrę). Zjawisko to nosi nazwę mode collaps. Poszukaj jak można rozwiązać ten problem."
      ],
      "metadata": {
        "id": "RyMjZW57iWQ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Aktualizowanie funkcji strat generatora w celu propagacji wstecznej;\n",
        "Niewprowadzone GAN- przewidywanie przyszłych dyskryminacji przy pomocy funkcji kosztu dyskryminatora\n",
        "2. Modyfikowanie dyskryminatora w celu podejmowania decyzji na podstawie wielu próbek tej samej klasy\n",
        "3. Strata Wassersteina- naprawa odległości między dwoma rozkładami prawdopodobieństwa.\n",
        "\n"
      ],
      "metadata": {
        "id": "CF5FFR8zj_hC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Zadanie5: CIFAR10 to dataset zawierający 60 tysięcy kolorowych obrazków rozmiaru 32 x 32 pikseli należących do 10 różnych klas. Poniżej przykładowe wizualizacje."
      ],
      "metadata": {
        "id": "SjAc8E7olh1-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nie wiem czy tego Pan oczekiwał w tym zadaniu, natomiast wykorzystałam implementację z ćwiczeń i innych źródeł. Kod jest podobny do kilku innych, dopasowany do moich danych. Nie widziałam innej opcji, ponieważ tak tworzy się sieć więc implementacja też tak powinna wyglądać (funkcje wymagają tych samych argumentów), nic nowego nie wymyślę :) Przynajmniej tak myślę :)\n",
        "Główny problem stanowiło coś przy przekształceniu liniowym..kompletnie nie wiedziałam jak go rozwiązać. Optymalizatory i funkcję kosztu wybrałam takie jak na ćwiczeniach."
      ],
      "metadata": {
        "id": "9bLl9p43gRAa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#biblioteki\n",
        "import torchvision\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.utils.data\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.datasets import CIFAR10\n",
        "from torchvision.transforms import ToTensor, Normalize, Compose\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "#używam GPU \n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "#DataLoader\n",
        "dataset = CIFAR10(root='.', train=True, transform=ToTensor(), download=True)\n",
        "loader = DataLoader(dataset, batch_size=15, shuffle=True)\n",
        "train_dataset= dataset\n",
        "test_dataset= dataset\n",
        "batch_size= 100\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
        "                                           batch_size=batch_size, \n",
        "                                           shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
        "                                          batch_size=batch_size, \n",
        "                                          shuffle=False)\n",
        "\n",
        "\n",
        "#implementacja sieci\n",
        "num_classes = 10\n",
        "class ConvCIFAR(nn.Module):                                             #tu korzystam z implementowanych na ćwiczeniach różnych kodów + z tego źródła:https://towardsdatascience.com/convolutional-neural-networks-explained-9cc5188c4939\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(ConvCIFAR, self).__init__()\n",
        "        self.layer1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=3, out_channels=48, kernel_size=5, stride=1, padding=2), \n",
        "            nn.BatchNorm2d(48),                               \n",
        "            nn.ReLU(),                                              \n",
        "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "        self.layer2 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=48, out_channels=96, kernel_size=5, stride=1, padding=2),\n",
        "            nn.BatchNorm2d(96),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "        self.fc = nn.Linear(21*21*96, num_classes)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        out = self.layer1(x)\n",
        "        out = self.layer2(out)\n",
        "        out = out.reshape(out.size(0), -1)\n",
        "        out = self.fc(out)  #nie rozumiem czemu tu wyskakuje błąd???\n",
        "        return out\n",
        "  \n",
        "#model\n",
        "model = ConvCIFAR(num_classes).to(device)\n",
        "\n",
        "#funckja kosztu i optymalizator\n",
        "learning_rate = 0.001\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "#Trenujemy model na danych\n",
        "num_epochs= 5\n",
        "total_step = len(train_loader)\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        \n",
        "        #Krok Forward\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        \n",
        "        #Krok Backward i optymalizacja\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        #co 100 przejscie wyswietl obecnna wartosc funkcji kosztu\n",
        "        if (i+1) % 100 == 0:\n",
        "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
        "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
        "\n",
        "model.eval() \n",
        "\n",
        "with torch.no_grad():\n",
        "    correct = 0 \n",
        "    total = 0  \n",
        "    for images, labels in test_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print(f'Test Accuracy of the model on the 60000 test images: {100 * correct / total}')\n",
        "\n",
        "#Zapisujemy nasz model\n",
        "torch.save(model.state_dict(), 'model.ckpt')\n",
        "\n"
      ],
      "metadata": {
        "id": "anv-Q_T6lpB-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZSjt5GP6WGUs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}